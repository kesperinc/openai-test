{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab 객체 생성\n",
    "m = Mecab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리할 파일들의 리스트를 가져옴\n",
    "file_list = ['1gapja.txt', '2gapsul.txt', '3gapsin.txt', '4gapo.txt', '5gapjin.txt', '6gapin.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별로 파일명과 챕터를 저장할 딕셔너리 생성\n",
    "word_info = {}\n",
    "\n",
    "# 특정 단어들을 설정\n",
    "specific_words = {'課體': set(), '課意': set(), '解曰': set(), '斷曰': set()}  # 특정 단어를 위한 집합\n",
    "\n",
    "# 숫자 및 원숫자 제거를 위한 정규 표현식\n",
    "number_pattern = re.compile(r'[0-9①②③④⑤⑥⑦⑧⑨⑩]+')\n",
    "\n",
    "# 한글(한자), 한글(한자): 패턴과 한자만 있는 경우를 처리하기 위한 정규 표현식\n",
    "hanja_hangul_pattern = re.compile(r'(\\w+)\\([一-龥]+\\)(:)?')\n",
    "hanja_only_pattern = re.compile(r'[一-龥]{2,}')\n",
    "\n",
    "# 챕터 패턴: 간지+일 제X국 형태를 매칭 (더 세부적으로 조정)\n",
    "chapter_pattern = re.compile(r'([甲乙丙丁戊己庚辛壬癸][子丑寅卯辰巳午未申酉戌亥]日\\s*第[一二三四五六七八九十百千]+局)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 챕터를 저장할 집합\n",
    "all_chapters = set()\n",
    "\n",
    "# 각 파일을 읽고 단어들을 추출하여 딕셔너리에 추가\n",
    "for file_name in file_list:\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "        # 파일의 현재 챕터를 추적하기 위한 변수\n",
    "        current_chapter = None\n",
    "        \n",
    "        # 숫자 및 원숫자를 먼저 제거\n",
    "        content = number_pattern.sub('', content)\n",
    "\n",
    "        # 챕터를 찾기\n",
    "        lines = content.splitlines()\n",
    "        for line in lines:\n",
    "            chapter_match = chapter_pattern.search(line)\n",
    "            if chapter_match:\n",
    "                current_chapter = chapter_match.group()\n",
    "                all_chapters.add(current_chapter)\n",
    "\n",
    "            # 한글(한자), 한글(한자): 패턴을 추출하여 저장\n",
    "            matches = hanja_hangul_pattern.findall(line)\n",
    "            for match in matches:\n",
    "                word = match[0]\n",
    "                if len(word) >= 2:  # 두 글자 이상인 경우만 저장\n",
    "                    if word not in word_info:\n",
    "                        word_info[word] = set()  # 챕터별로 저장하기 위해 set 사용\n",
    "                    word_info[word].add(current_chapter)\n",
    "                    if word in specific_words:\n",
    "                        specific_words[word].add(current_chapter)\n",
    "        \n",
    "            # 한자만 있는 경우를 처리하여 저장\n",
    "            matches_hanja_only = hanja_only_pattern.findall(line)\n",
    "            for match in matches_hanja_only:\n",
    "                if len(match) >= 2:  # 두 글자 이상인 경우만 저장\n",
    "                    if match not in word_info:\n",
    "                        word_info[match] = set()  # 챕터별로 저장하기 위해 set 사용\n",
    "                    word_info[match].add(current_chapter)\n",
    "                    if match in specific_words:\n",
    "                        specific_words[match].add(current_chapter)\n",
    "\n",
    "            # 한글(한자) 및 한자 패턴을 제거한 나머지 텍스트에 대해 형태소 분석\n",
    "            line_cleaned = hanja_hangul_pattern.sub('', line)\n",
    "            line_cleaned = hanja_only_pattern.sub('', line)\n",
    "            nouns = m.nouns(line_cleaned)\n",
    "            \n",
    "            # 2글자 이상인 명사만 필터링하여 저장\n",
    "            for noun in nouns:\n",
    "                if len(noun) >= 2:\n",
    "                    if noun not in word_info:\n",
    "                        word_info[noun] = set()  # 챕터별로 저장하기 위해 set 사용\n",
    "                    word_info[noun].add(current_chapter)\n",
    "                    if noun in specific_words:\n",
    "                        specific_words[noun].add(current_chapter)\n",
    "\n",
    "# 각 단어가 등장한 챕터 수를 계산\n",
    "word_chapter_counts = {word: len(chapters) for word, chapters in word_info.items()}\n",
    "\n",
    "# 등장 횟수에 따라 내림차순으로 정렬\n",
    "sorted_words_by_count = sorted(word_chapter_counts.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 단어가 등장한 챕터 수에 따라 정렬된 결과와 특정 단어의 챕터 수 확인 결과가 jikji_frequency_sorted.txt 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 결과를 출력 및 저장\n",
    "with open('jikji_frequency_sorted.txt', 'w', encoding='utf-8') as result_file:\n",
    "    for word, count in sorted_words_by_count:\n",
    "        result_file.write(f\"{word}: {count}개 챕터에서 등장\\n\")\n",
    "\n",
    "    # 특정 단어의 챕터 수와 720개 챕터가 아닌 경우 출력\n",
    "    result_file.write(\"\\n특정 단어의 챕터 수 확인:\\n\")\n",
    "    for word, chapters in specific_words.items():\n",
    "        count = len(chapters)\n",
    "        result_file.write(f\"{word}: {count}개 챕터에서 등장\\n\")\n",
    "        if count != 720:\n",
    "            missing_chapters = all_chapters - chapters\n",
    "            result_file.write(f\"  [경고] {word}가 누락된 챕터들 ({len(missing_chapters)}개):\\n\")\n",
    "            for chapter in sorted(missing_chapters):\n",
    "                result_file.write(f\"    - {chapter}\\n\")\n",
    "\n",
    "print(\"각 단어가 등장한 챕터 수에 따라 정렬된 결과와 특정 단어의 챕터 수 확인 결과가 jikji_frequency_sorted.txt 파일에 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
